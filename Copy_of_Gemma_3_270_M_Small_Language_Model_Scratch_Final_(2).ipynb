{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhianvSmeena/Small-Language-Model/blob/main/Copy_of_Gemma_3_270_M_Small_Language_Model_Scratch_Final_(2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Vizuara Presents: Build Gemma3 270M from Scratch"
      ],
      "metadata": {
        "id": "m7r4kNuFQTbJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijDG3IDqkjKx"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "Let us build a Small Language Model (SLM) from scratch. We will try to keep the parameter size to 50-60 million.\n",
        "\n",
        "Our goal is to generate creative and coherent text based on the input data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPl23BNsRqfm"
      },
      "source": [
        "## Step 1: Import the Dataset\n",
        "\n",
        "TinyStories is a synthetic dataset of short stories that only contain words that a typical 3 to 4-year-olds usually understand, generated by GPT-3.5 and GPT-4. We can get it from HuggingFace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BSeXbOztRtKN"
      },
      "outputs": [],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AkpPWqR8-tFO"
      },
      "outputs": [],
      "source": [
        "pip install -U datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3r34TTsnR3GI"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"roneneldan/TinyStories\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vccyr4qKR6OH"
      },
      "source": [
        "## Step 2: Tokenize the Dataset\n",
        "\n",
        "In this step, we will do the following:\n",
        "\n",
        "(1) Tokenize the dataset into tokenIDs.\n",
        "\n",
        "(2) Create a file called \"train.bin\" and \"validtion.bin\" where we will store the tokenIDs from the entire dataset.\n",
        "\n",
        "(3) We make sure the tokenIDs are stored on a disk, rather than on the RAM for efficient computations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFkgAjyMR8fa"
      },
      "outputs": [],
      "source": [
        "!pip install tiktoken\n",
        "import tiktoken\n",
        "import os\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "# Some functions from https://github.com/karpathy/nanoGPT/blob/master/data/openwebtext/prepare.py\n",
        "\n",
        "def process(example):\n",
        "    ids = enc.encode_ordinary(example['text']) # encode_ordinary ignores any special tokens\n",
        "    out = {'ids': ids, 'len': len(ids)}\n",
        "    return out\n",
        "\n",
        "if not os.path.exists(\"train.bin\"):\n",
        "    tokenized = ds.map(\n",
        "        process,\n",
        "        remove_columns=['text'],\n",
        "        desc=\"tokenizing the splits\",\n",
        "        num_proc=8,\n",
        "        )\n",
        "    # concatenate all the ids in each dataset into one large file we can use for training\n",
        "    for split, dset in tokenized.items():\n",
        "        arr_len = np.sum(dset['len'], dtype=np.uint64)\n",
        "        filename = f'{split}.bin'\n",
        "        dtype = np.uint16 # (can do since enc.max_token_value == 50256 is < 2**16)\n",
        "        arr = np.memmap(filename, dtype=dtype, mode='w+', shape=(arr_len,))\n",
        "        total_batches = 1024\n",
        "\n",
        "        idx = 0\n",
        "        for batch_idx in tqdm(range(total_batches), desc=f'writing {filename}'):\n",
        "            # Batch together samples for faster write\n",
        "            batch = dset.shard(num_shards=total_batches, index=batch_idx, contiguous=True).with_format('numpy')\n",
        "            arr_batch = np.concatenate(batch['ids'])\n",
        "            # Write into mmap\n",
        "            arr[idx : idx + len(arr_batch)] = arr_batch\n",
        "            idx += len(arr_batch)\n",
        "        arr.flush()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_qRtn_WSbV4"
      },
      "source": [
        "## Step 3: Create Input-Output batches for the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gak79CZESkjN"
      },
      "outputs": [],
      "source": [
        "# Some functions from https://github.com/karpathy/nanoGPT/blob/master/train.py with slight modifications\n",
        "#block size = context window\n",
        "def get_batch(split):\n",
        "    # We recreate np.memmap every batch to avoid a memory leak, as per\n",
        "    # https://stackoverflow.com/questions/45132940/numpy-memmap-memory-usage-want-to-iterate-once/61472122#61472122\n",
        "    if split == 'train':\n",
        "        data = np.memmap('train.bin', dtype=np.uint16, mode='r')\n",
        "    else:\n",
        "        data = np.memmap('validation.bin', dtype=np.uint16, mode='r')\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
        "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
        "    if device_type == 'cuda':\n",
        "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
        "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
        "    else:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "    return x, y\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kA1SVp1hkjKy"
      },
      "source": [
        "## Step 4: Define the SLM Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DsjXJXs2FVJu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "def compute_rope_params(head_dim, theta_base=10_000, context_length=4096, dtype=torch.float32):\n",
        "    assert head_dim % 2 == 0, \"Embedding dimension must be even\"\n",
        "\n",
        "    # Compute the inverse frequencies\n",
        "    inv_freq = 1.0 / (theta_base ** (torch.arange(0, head_dim, 2, dtype=dtype)[: (head_dim // 2)].float() / head_dim))\n",
        "\n",
        "    # Generate position indices\n",
        "    positions = torch.arange(context_length, dtype=dtype)\n",
        "\n",
        "    # Compute the angles\n",
        "    angles = positions[:, None] * inv_freq[None, :]  # Shape: (context_length, head_dim // 2)\n",
        "\n",
        "    # Expand angles to match the head_dim\n",
        "    angles = torch.cat([angles, angles], dim=1)  # Shape: (context_length, head_dim)\n",
        "\n",
        "    # Precompute sine and cosine\n",
        "    cos = torch.cos(angles)\n",
        "    sin = torch.sin(angles)\n",
        "\n",
        "    return cos, sin\n",
        "\n",
        "\n",
        "def apply_rope(x, cos, sin):\n",
        "    # x: (batch_size, num_heads, seq_len, head_dim)\n",
        "    batch_size, num_heads, seq_len, head_dim = x.shape\n",
        "    assert head_dim % 2 == 0, \"Head dimension must be even\"\n",
        "\n",
        "    # Split x into first half and second half\n",
        "    x1 = x[..., : head_dim // 2]  # First half\n",
        "    x2 = x[..., head_dim // 2 :]  # Second half\n",
        "\n",
        "    # Adjust sin and cos shapes\n",
        "    cos = cos[:seq_len, :].unsqueeze(0).unsqueeze(0)  # Shape: (1, 1, seq_len, head_dim)\n",
        "    sin = sin[:seq_len, :].unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "    # Apply the rotary transformation\n",
        "    rotated = torch.cat((-x2, x1), dim=-1)\n",
        "    x_rotated = (x * cos) + (rotated * sin)\n",
        "\n",
        "    # It's ok to use lower-precision after applying cos and sin rotation\n",
        "    return x_rotated.to(dtype=x.dtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9friaxWABOA-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from dataclasses import dataclass\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "from contextlib import nullcontext\n",
        "import os\n",
        "\n",
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, emb_dim, eps=1e-6, bias=False):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        # Gemma3 stores zero-centered weights and uses (1 + weight) during forward\n",
        "        self.scale = nn.Parameter(torch.zeros(emb_dim))\n",
        "        self.shift = nn.Parameter(torch.zeros(emb_dim)) if bias else None\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Match HF Gemma3: compute norm in float32, then scale by (1 + w)\n",
        "        input_dtype = x.dtype\n",
        "        x_f = x.float()\n",
        "        var = x_f.pow(2).mean(dim=-1, keepdim=True)\n",
        "        x_norm = x_f * torch.rsqrt(var + self.eps)\n",
        "        out = x_norm * (1.0 + self.scale.float())\n",
        "\n",
        "        if self.shift is not None:\n",
        "            out = out + self.shift.float()\n",
        "\n",
        "        return out.to(input_dtype)\n",
        "\n",
        "\n",
        "\n",
        "class GroupedQueryAttention(nn.Module):\n",
        "    def __init__(\n",
        "        self, d_in, num_heads, num_kv_groups, head_dim=None, qk_norm=False,\n",
        "        query_pre_attn_scalar=None, dtype=None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        assert num_heads % num_kv_groups == 0, \"num_heads must be divisible by num_kv_groups\"\n",
        "\n",
        "        self.num_heads = num_heads\n",
        "        self.num_kv_groups = num_kv_groups\n",
        "        self.group_size = num_heads // num_kv_groups\n",
        "\n",
        "        if head_dim is None:\n",
        "            assert d_in % num_heads == 0, \"`d_in` must be divisible by `num_heads` if `head_dim` is not set\"\n",
        "            head_dim = d_in // num_heads\n",
        "\n",
        "        self.head_dim = head_dim\n",
        "        self.d_out = num_heads * head_dim\n",
        "\n",
        "        self.W_query = nn.Linear(d_in, self.d_out, bias=False, dtype=dtype)\n",
        "        self.W_key = nn.Linear(d_in, num_kv_groups * head_dim, bias=False, dtype=dtype)\n",
        "        self.W_value = nn.Linear(d_in, num_kv_groups * head_dim, bias=False, dtype=dtype)\n",
        "\n",
        "        self.out_proj = nn.Linear(self.d_out, d_in, bias=False, dtype=dtype)\n",
        "\n",
        "        if qk_norm:\n",
        "            self.q_norm = RMSNorm(head_dim, eps=1e-6)\n",
        "            self.k_norm = RMSNorm(head_dim, eps=1e-6)\n",
        "        else:\n",
        "            self.q_norm = self.k_norm = None\n",
        "\n",
        "        if query_pre_attn_scalar is not None:\n",
        "            self.scaling = (query_pre_attn_scalar) ** -0.5\n",
        "        else:\n",
        "            self.scaling = (head_dim) ** -0.5\n",
        "\n",
        "\n",
        "    def forward(self, x, mask, cos, sin):\n",
        "        b, num_tokens, _ = x.shape\n",
        "\n",
        "        # Apply projections\n",
        "        queries = self.W_query(x)  # (b, num_tokens, num_heads * head_dim)\n",
        "        keys = self.W_key(x)       # (b, num_tokens, num_kv_groups * head_dim)\n",
        "        values = self.W_value(x)   # (b, num_tokens, num_kv_groups * head_dim)\n",
        "\n",
        "        # Reshape\n",
        "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        keys = keys.view(b, num_tokens, self.num_kv_groups, self.head_dim).transpose(1, 2)\n",
        "        values = values.view(b, num_tokens, self.num_kv_groups, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # Optional normalization\n",
        "        if self.q_norm:\n",
        "            queries = self.q_norm(queries)\n",
        "        if self.k_norm:\n",
        "            keys = self.k_norm(keys)\n",
        "\n",
        "        # Apply RoPE\n",
        "        queries = apply_rope(queries, cos, sin)\n",
        "        keys = apply_rope(keys, cos, sin)\n",
        "\n",
        "        # Expand K and V to match number of heads\n",
        "        keys = keys.repeat_interleave(self.group_size, dim=1)\n",
        "        values = values.repeat_interleave(self.group_size, dim=1)\n",
        "\n",
        "        # Scale queries\n",
        "        queries = queries * self.scaling\n",
        "\n",
        "        # Attention\n",
        "        attn_scores = queries @ keys.transpose(2, 3)\n",
        "        attn_scores = attn_scores.masked_fill(mask, -torch.inf)\n",
        "        attn_weights = torch.softmax(attn_scores, dim=-1)\n",
        "\n",
        "        context = (attn_weights @ values).transpose(1, 2).reshape(b, num_tokens, self.d_out)\n",
        "        return self.out_proj(context)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(cfg[\"emb_dim\"], cfg[\"hidden_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
        "        self.fc2 = nn.Linear(cfg[\"emb_dim\"], cfg[\"hidden_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
        "        self.fc3 = nn.Linear(cfg[\"hidden_dim\"], cfg[\"emb_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_fc1 = self.fc1(x)\n",
        "        x_fc2 = self.fc2(x)\n",
        "        x = nn.functional.gelu(x_fc1, approximate=\"tanh\") * x_fc2\n",
        "        return self.fc3(x)\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, cfg: dict, attn_type: str):\n",
        "        super().__init__()\n",
        "        self.attn_type = attn_type\n",
        "\n",
        "        self.att = GroupedQueryAttention(\n",
        "            d_in=cfg[\"emb_dim\"],\n",
        "            num_heads=cfg[\"n_heads\"],\n",
        "            num_kv_groups=cfg[\"n_kv_groups\"],\n",
        "            head_dim=cfg[\"head_dim\"],\n",
        "            qk_norm=cfg[\"qk_norm\"],\n",
        "            query_pre_attn_scalar=cfg[\"query_pre_attn_scalar\"],\n",
        "            dtype=cfg[\"dtype\"],\n",
        "        )\n",
        "        self.ff = FeedForward(cfg)\n",
        "        self.input_layernorm = RMSNorm(cfg[\"emb_dim\"], eps=1e-6)\n",
        "        self.post_attention_layernorm = RMSNorm(cfg[\"emb_dim\"], eps=1e-6)\n",
        "        self.pre_feedforward_layernorm = RMSNorm(cfg[\"emb_dim\"], eps=1e-6)\n",
        "        self.post_feedforward_layernorm = RMSNorm(cfg[\"emb_dim\"], eps=1e-6)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x,\n",
        "        mask_global,\n",
        "        mask_local,\n",
        "        cos_global,\n",
        "        sin_global,\n",
        "        cos_local,\n",
        "        sin_local,\n",
        "    ):\n",
        "        # Shortcut connection for attention block\n",
        "        shortcut = x\n",
        "        x = self.input_layernorm(x)\n",
        "\n",
        "        if self.attn_type == \"sliding_attention\":\n",
        "            attn_mask = mask_local\n",
        "            cos = cos_local\n",
        "            sin = sin_local\n",
        "        else:\n",
        "            attn_mask = mask_global\n",
        "            cos = cos_global\n",
        "            sin = sin_global\n",
        "\n",
        "        x_attn = self.att(x, attn_mask, cos, sin)\n",
        "        x_attn = self.post_attention_layernorm(x_attn)\n",
        "        x = shortcut + x_attn\n",
        "\n",
        "        # Shortcut connection for feed forward block\n",
        "        shortcut = x\n",
        "        x_ffn = self.pre_feedforward_layernorm(x)\n",
        "        x_ffn = self.ff(x_ffn)\n",
        "        x_ffn = self.post_feedforward_layernorm(x_ffn)\n",
        "        x = shortcut + x_ffn\n",
        "        return x\n",
        "\n",
        "class Gemma3Model(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        assert cfg[\"layer_types\"] is not None and len(cfg[\"layer_types\"]) == cfg[\"n_layers\"]\n",
        "\n",
        "        # Main model parameters\n",
        "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"], dtype=cfg[\"dtype\"])\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerBlock(cfg, attn_type)for attn_type in cfg[\"layer_types\"]\n",
        "        ])\n",
        "\n",
        "        self.final_norm = RMSNorm(cfg[\"emb_dim\"], eps=1e-6)\n",
        "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False, dtype=cfg[\"dtype\"])\n",
        "        self.cfg = cfg\n",
        "\n",
        "        # Reusuable utilities\n",
        "        cos_local, sin_local = compute_rope_params(\n",
        "            head_dim=cfg[\"head_dim\"],\n",
        "            theta_base=cfg[\"rope_local_base\"],\n",
        "            context_length=cfg[\"context_length\"],\n",
        "            dtype=torch.float32,\n",
        "        )\n",
        "        cos_global, sin_global = compute_rope_params(\n",
        "            head_dim=cfg[\"head_dim\"],\n",
        "            theta_base=cfg[\"rope_base\"],\n",
        "            context_length=cfg[\"context_length\"],\n",
        "            dtype=torch.float32,\n",
        "        )\n",
        "        self.register_buffer(\"cos_local\", cos_local, persistent=False)\n",
        "        self.register_buffer(\"sin_local\", sin_local, persistent=False)\n",
        "        self.register_buffer(\"cos_global\", cos_global, persistent=False)\n",
        "        self.register_buffer(\"sin_global\", sin_global, persistent=False)\n",
        "\n",
        "    def _create_masks(self, seq_len, device):\n",
        "        ones = torch.ones((seq_len, seq_len), dtype=torch.bool, device=device)\n",
        "\n",
        "        # mask_global (future is masked: j > i)\n",
        "        #     j:  0 1 2 3 4 5 6 7\n",
        "        #  i\n",
        "        #     0:  0 1 1 1 1 1 1 1\n",
        "        #     1:  0 0 1 1 1 1 1 1\n",
        "        #     2:  0 0 0 1 1 1 1 1\n",
        "        #     3:  0 0 0 0 1 1 1 1\n",
        "        #     4:  0 0 0 0 0 1 1 1\n",
        "        #     5:  0 0 0 0 0 0 1 1\n",
        "        #     6:  0 0 0 0 0 0 0 1\n",
        "        #     7:  0 0 0 0 0 0 0 0\n",
        "        mask_global = torch.triu(ones, diagonal=1)\n",
        "\n",
        "        # far_past (too far back is masked: i - j >= sliding_window)\n",
        "        # where sliding_window = 4\n",
        "        #     j:  0 1 2 3 4 5 6 7\n",
        "        #  i\n",
        "        #     0:  0 0 0 0 0 0 0 0\n",
        "        #     1:  0 0 0 0 0 0 0 0\n",
        "        #     2:  0 0 0 0 0 0 0 0\n",
        "        #     3:  0 0 0 0 0 0 0 0\n",
        "        #     4:  1 0 0 0 0 0 0 0\n",
        "        #     5:  1 1 0 0 0 0 0 0\n",
        "        #     6:  1 1 1 0 0 0 0 0\n",
        "        #     7:  1 1 1 1 0 0 0 0\n",
        "        far_past = torch.triu(ones, diagonal=self.cfg[\"sliding_window\"]).T\n",
        "\n",
        "        # Local (sliding_window) = future OR far-past\n",
        "        # mask_local\n",
        "        #     j:  0 1 2 3 4 5 6 7\n",
        "        # i\n",
        "        # 0:      0 1 1 1 1 1 1 1\n",
        "        # 1:      0 0 1 1 1 1 1 1\n",
        "        # 2:      0 0 0 1 1 1 1 1\n",
        "        # 3:      0 0 0 0 1 1 1 1\n",
        "        # 4:      1 0 0 0 0 1 1 1\n",
        "        # 5:      1 1 0 0 0 0 1 1\n",
        "        # 6:      1 1 1 0 0 0 0 1\n",
        "        # 7:      1 1 1 1 0 0 0 0\n",
        "        mask_local = mask_global | far_past\n",
        "        return mask_global, mask_local\n",
        "\n",
        "    def forward(self, input_ids, targets=None):\n",
        "        b, seq_len = input_ids.shape\n",
        "        x = self.tok_emb(input_ids) * (self.cfg[\"emb_dim\"] ** 0.5)\n",
        "        mask_global, mask_local = self._create_masks(seq_len, x.device)\n",
        "\n",
        "        for block in self.blocks:\n",
        "            x = block(\n",
        "                x,\n",
        "                mask_global=mask_global,\n",
        "                mask_local=mask_local,\n",
        "                cos_global=self.cos_global,\n",
        "                sin_global=self.sin_global,\n",
        "                cos_local=self.cos_local,\n",
        "                sin_local=self.sin_local,\n",
        "            )\n",
        "\n",
        "        x = self.final_norm(x)\n",
        "        logits = self.out_head(x.to(self.cfg[\"dtype\"]))\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)), targets.reshape(-1))\n",
        "        return logits, loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
        "      for _ in range(max_new_tokens):\n",
        "        ctx_len = self.cfg[\"context_length\"]\n",
        "        idx_cond = idx if idx.size(1) <= ctx_len else idx[:, -ctx_len:]\n",
        "        logits, _ = self(idx_cond)  # targets=None by default\n",
        "        logits = logits[:, -1, :] / temperature\n",
        "        if top_k is not None:\n",
        "            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "            logits[logits < v[:, [-1]]] = float(\"-inf\")\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        idx_next = torch.multinomial(probs, num_samples=1)\n",
        "        idx = torch.cat((idx, idx_next), dim=1)\n",
        "      return idx\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRm6WlvfkjKz"
      },
      "outputs": [],
      "source": [
        "GEMMA3_CONFIG_270M = {\n",
        "    \"vocab_size\": 50257,\n",
        "    \"context_length\": 32_768,\n",
        "    \"emb_dim\": 640,\n",
        "    \"n_heads\": 4,\n",
        "    \"n_layers\": 18,\n",
        "    \"hidden_dim\": 2048,\n",
        "    \"head_dim\": 256,\n",
        "    \"qk_norm\": True,\n",
        "    \"n_kv_groups\": 1,\n",
        "    \"rope_local_base\": 10_000.0,\n",
        "    \"rope_base\": 1_000_000.0,\n",
        "    \"sliding_window\": 512,\n",
        "      \"layer_types\": [\n",
        "        \"sliding_attention\",\n",
        "        \"sliding_attention\",\n",
        "        \"sliding_attention\",\n",
        "        \"sliding_attention\",\n",
        "        \"sliding_attention\",\n",
        "        \"full_attention\",\n",
        "        \"sliding_attention\",\n",
        "        \"sliding_attention\",\n",
        "        \"sliding_attention\",\n",
        "        \"sliding_attention\",\n",
        "        \"sliding_attention\",\n",
        "        \"full_attention\",\n",
        "        \"sliding_attention\",\n",
        "        \"sliding_attention\",\n",
        "        \"sliding_attention\",\n",
        "        \"sliding_attention\",\n",
        "        \"sliding_attention\",\n",
        "        \"full_attention\"\n",
        "    ],\n",
        "    \"dtype\": torch.bfloat16,\n",
        "    \"query_pre_attn_scalar\": 256,\n",
        "}\n",
        "\n",
        "torch.manual_seed(123)\n",
        "model = Gemma3Model(GEMMA3_CONFIG_270M)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_a8Rd-0S_WC"
      },
      "source": [
        "## Step 5: Define the loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "La2Aun_nTBzk"
      },
      "outputs": [],
      "source": [
        "def estimate_loss(model):\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "        for split in ['train', 'val']:\n",
        "            losses = torch.zeros(eval_iters)\n",
        "            for k in range(eval_iters):\n",
        "                X, Y = get_batch(split)\n",
        "                with ctx:\n",
        "                    logits, loss = model(X, Y)\n",
        "                losses[k] = loss.item()\n",
        "            out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvqWPUstTRXO"
      },
      "source": [
        "## Step 6: Define SLM Training Configuration Part 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8QQyayhnkjK5"
      },
      "outputs": [],
      "source": [
        "# Training Config\n",
        "import torch\n",
        "from contextlib import nullcontext\n",
        "\n",
        "learning_rate = 1e-4 #more stable training, earlier 1e-4\n",
        "max_iters = 150000 #increase from 25000\n",
        "warmup_steps = 1000 #smoother initial train, earlier 100\n",
        "min_lr = 5e-4 #lower rate, earlier 5e-4\n",
        "eval_iters = 500 # increased from 100\n",
        "batch_size = 32 # changed from 16, better gradient estimate\n",
        "block_size = 128 #changed from 64, capture longer range dependencies\n",
        "\n",
        "gradient_accumulation_steps = 32 # reduced from 50\n",
        "\n",
        "device =  \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
        "# note: float16 data type will automatically use a GradScaler\n",
        "\n",
        "# How to use autocast https://wandb.ai/wandb_fc/tips/reports/How-To-Use-Autocast-in-PyTorch--VmlldzoyMTk4NTky\n",
        "#dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
        "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
        "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
        "\n",
        "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
        "\n",
        "torch.set_default_device(device)\n",
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmWj6YcKTW_z"
      },
      "source": [
        "## Step 7: Define SLM Training Configuration Part 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMkO3FlNkjK6"
      },
      "outputs": [],
      "source": [
        "from torch.optim.lr_scheduler import LinearLR,SequentialLR, CosineAnnealingLR\n",
        "\n",
        "##PUT IN WEIGHT DECAY, CHANGED BETA2 to 0.95\n",
        "optimizer =  torch.optim.AdamW(model.parameters(), lr=learning_rate, betas=(0.9, 0.95), weight_decay=0.1, eps=1e-9) #weight decay for regularization\n",
        "\n",
        "scheduler_warmup = LinearLR(optimizer, total_iters = warmup_steps) #Implement linear warmup\n",
        "scheduler_decay = CosineAnnealingLR(optimizer,T_max = max_iters - warmup_steps, eta_min = min_lr) #Implement lr decay\n",
        "scheduler = SequentialLR(optimizer, schedulers=[scheduler_warmup, scheduler_decay], milestones=[warmup_steps]) #Switching from warmup to decay\n",
        "\n",
        "# https://stackoverflow.com/questions/72534859/is-gradscaler-necessary-with-mixed-precision-training-with-pytorch\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nz8fPSKNTY3W"
      },
      "source": [
        "## Step 8: Pre-train the SLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t0l-YhockjK6"
      },
      "outputs": [],
      "source": [
        "best_val_loss = float('inf')\n",
        "best_model_params_path = \"best_model_params.pt\"\n",
        "train_loss_list, validation_loss_list = [], []\n",
        "\n",
        "# Ensure model is on the correct device\n",
        "model = model.to(device)\n",
        "\n",
        "# In your training loop\n",
        "for epoch in tqdm(range(max_iters)):\n",
        "    if epoch % eval_iters == 0 and epoch != 0:\n",
        "        # Ensure estimate_loss uses the correct device\n",
        "        losses = estimate_loss(model)\n",
        "        print(f\"Epoch {epoch}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "        print(f\"The current learning rate: {optimizer.param_groups[0]['lr']:.5f}\")\n",
        "        train_loss_list += [losses['train']]\n",
        "        validation_loss_list += [losses['val']]\n",
        "\n",
        "        if losses['val'] < best_val_loss:\n",
        "            best_val_loss = losses['val']\n",
        "            torch.save(model.state_dict(), best_model_params_path)\n",
        "\n",
        "    # Ensure X and y are on the correct device\n",
        "    X, y = get_batch(\"train\")\n",
        "    X, y = X.to(device), y.to(device)\n",
        "\n",
        "    with ctx:\n",
        "        logits, loss = model(X, y)\n",
        "        loss = loss / gradient_accumulation_steps\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "    if ((epoch + 1) % gradient_accumulation_steps == 0) or (epoch + 1 == max_iters):\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "    scheduler.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdzhSo_7TcgI"
      },
      "source": [
        "## Step 9: Plot the SLM Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KSWpvAnakjK6"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "train_loss_list_converted = [i.cpu().detach() for i in train_loss_list]\n",
        "validation_loss_list_converted = [i.cpu().detach() for i in validation_loss_list]\n",
        "\n",
        "plt.plot(train_loss_list_converted, 'g', label='train_loss')\n",
        "plt.plot(validation_loss_list_converted, 'r', label='validation_loss')\n",
        "plt.xlabel(\"Steps - Every 100 epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2ocyyQwkjK6"
      },
      "source": [
        "## Step 10: Run SLM Inference on our trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06NrdWKdkjK7"
      },
      "outputs": [],
      "source": [
        "#Load the model\n",
        "model = Gemma3Model(GEMMA3_CONFIG_270M)  # re-create the model with same config\n",
        "device =  \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "best_model_params_path = \"best_model_params.pt\"\n",
        "model.load_state_dict(torch.load(best_model_params_path, map_location=torch.device(device))) # load best model states\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8PgWXb-kjK7"
      },
      "outputs": [],
      "source": [
        "sentence = \"Once upon a time there was a pumpkin.\"\n",
        "context = (torch.tensor(enc.encode_ordinary(sentence)).unsqueeze(dim = 0))\n",
        "y = model.generate(context, 200)\n",
        "print(enc.decode(y.squeeze().tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZsL_mONRZAO"
      },
      "outputs": [],
      "source": [
        "sentence = \"A little girl went to the woods\"\n",
        "context = (torch.tensor(enc.encode_ordinary(sentence)).unsqueeze(dim = 0))\n",
        "y = model.generate(context, 200)\n",
        "print(enc.decode(y.squeeze().tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xu0MOoxmf6oo"
      },
      "outputs": [],
      "source": [
        "sentence = \"Grandmother was telling the kids story about a unicorn\"\n",
        "context = (torch.tensor(enc.encode_ordinary(sentence)).unsqueeze(dim = 0))\n",
        "y = model.generate(context, 200)\n",
        "print(enc.decode(y.squeeze().tolist()))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30732,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}